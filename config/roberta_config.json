{
"pretrain_params": {
                    "num_epochs": 10, 
                    "lr": 0.0001, 
                    "batch_size": 16,
                    "early_stop_threshold": 3
                    }, 
"finetune_params": {
                    "num_epochs": 10, 
                    "batch_size": 16, 
                    "early_stop_threshold": 3,
                    "warmup_steps": 0,
                    "optimizer": "AdamW",
                    "model_head": "mlp",
                    "loss_fn": "rmse"
                    }, 
"roberta_config": {
                    "vocab_size": 30522, 
                    "max_position_embeddings": 768,
                    "hidden_size": 768, 
                    "num_attention_heads": 12, 
                    "num_hidden_layers": 12, 
                    "type_vocab_size": 1
                    }
}