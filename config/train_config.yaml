
pretrain_params:
  num_epochs: 10
  batch_size: 16
  lr: 0.0001
  early_stop_threshold: 3
  model_head: multilabel

finetune_params:
  num_epochs: 10
  batch_size: 16
  lr: 5e-7
  early_stop_threshold: 3
  warmup_steps: 0
  optimizer: AdamW
  model_head: mlp
  loss_fn: rmse

roberta_config:
  vocab_size: 30522
  max_position_embeddings: 768
  hidden_size: 768
  num_attention_heads: 12
  num_hidden_layers: 12
  type_vocab_size: 1
