params:
  num_epochs: 60
  batch_size: 16
  lr: 1.0e-6
  early_stop_threshold: 20
  scheduler: linear # constant, linear
  warmup_steps: 0 
  optimizer: gLLRD  # AdamW, gLLRD
  model_head: pooler    # pooler, regressor1~3
  loss_fn: mae     # mae, rmse, L2, smooth_l1

paths:
  train_data: "data/df_train.pkl" 
  val_data: "data/df_val.pkl" 
  pt_ckpt: "roberta-base" # or path to checkpoint
  tknz: "roberta-base" # or custom tokenizer