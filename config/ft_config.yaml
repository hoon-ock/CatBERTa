params:
  num_epochs: 10
  batch_size: 16
  lr: 5.0e-7
  early_stop_threshold: 3
  scheduler: linear # constant, linear
  warmup_steps: 0
  optimizer: gLLRD    # AdamW, gLLRD
  model_head: mlp     # pooler, mlp, mlp2, attnhead, concatlayer
  loss_fn: rmse       # mae, rmse, L2

paths:
  train_data: "./data/df_train.pkl"
  val_data: "./data/df_val.pkl"
  ft_config: "./config/ft_config.yaml"
  pt_ckpt: "./checkpoint/pretrain/pt_0618_0343"
  tknz: "./tokenizer"